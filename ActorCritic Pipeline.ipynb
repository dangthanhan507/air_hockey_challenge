{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "715a333c-00f4-4977-9e83-3c6044de2796",
   "metadata": {},
   "source": [
    "# Actor-Critic Training Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5373eaa5-ac74-46d7-838f-6d410e889ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#the usual suspects for importing\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "#general imports\n",
    "from air_hockey_challenge.framework import AgentBase\n",
    "from air_hockey_challenge.framework.air_hockey_challenge_wrapper import AirHockeyChallengeWrapper\n",
    "from air_hockey_challenge.framework.challenge_core import ChallengeCore\n",
    "\n",
    "\n",
    "#approximator, optimizer, and policy imports\n",
    "from mushroom_rl.policy.gaussian_policy import GaussianPolicy\n",
    "from mushroom_rl.approximators.parametric import TorchApproximator\n",
    "from mushroom_rl.utils.optimizers import AdaptiveOptimizer\n",
    "from mushroom_rl.policy import OrnsteinUhlenbeckPolicy\n",
    "\n",
    "\n",
    "#challenge core imports\n",
    "import time\n",
    "from mushroom_rl.core import Core\n",
    "from air_hockey_challenge.framework.challenge_core import ChallengeCore\n",
    "from air_hockey_challenge.framework.agent_base import AgentBase\n",
    "\n",
    "#actor critic algorithm\n",
    "from mushroom_rl.algorithms.actor_critic import DDPG, TD3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08137d86-49a3-470b-a252-4a2e9bedb0a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.10.11\n"
     ]
    }
   ],
   "source": [
    "!python3 --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1aa44840-452e-487a-a221-fe8828ed8536",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/andang/anaconda3/envs/eecs298/bin/jupyter\n"
     ]
    }
   ],
   "source": [
    "!which jupyter lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4c31942-ac92-453b-b2ee-def728375fa4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65888f51-0fb9-4dc6-b75a-8ca7b8acc2e6",
   "metadata": {},
   "source": [
    "## Deploying these neural networks for ActorCritic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c14821d9-682d-4c34-863c-a5604835d440",
   "metadata": {},
   "source": [
    "### Notes for networks\n",
    "- Actor Network:\n",
    "    - takes in observation\n",
    "    - outputs action\n",
    "- Critic Network:\n",
    "    - takes in action and observation\n",
    "    - outputs a value (critic value shape 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9625fd2-5993-4e26-8633-5f69c2dfc61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActionGenerator(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, use_cuda = False, dropout=False, activation = nn.LeakyReLU(0.1) ):\n",
    "        super().__init__()\n",
    "        \n",
    "        num_layers = 20\n",
    "        layer_width = 10\n",
    "        \n",
    "        input_dim = input_dim[0]\n",
    "        output_dim = output_dim[0]\n",
    "        \n",
    "        layers = [nn.Linear(input_dim, layer_width), activation]\n",
    "        for i in range(num_layers-1):\n",
    "            layers.append(nn.Linear(layer_width, layer_width))\n",
    "            layers.append(activation)\n",
    "        layers.append(nn.Linear(layer_width, output_dim))\n",
    "        layers.append(activation)\n",
    "        \n",
    "        self.model = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, obs):\n",
    "        out = self.model(obs.float())\n",
    "        \n",
    "        # return torch.reshape(out,6)\n",
    "        return out\n",
    "\n",
    "class CriticNetwork(nn.Module):\n",
    "    def __init__(self, input_shape, output_shape,**kwargs):\n",
    "        '''\n",
    "            Arguments:\n",
    "            ----------\n",
    "                input_shape: (m,) wh#ere m is length of input\n",
    "                output_shape: (m,n,..) where it represents shape of output\n",
    "        '''\n",
    "        super().__init__()\n",
    "        \n",
    "        input_dim = input_shape[0]\n",
    "        output_dim = 1\n",
    "        for i in output_shape: \n",
    "            output_dim *= i\n",
    "        \n",
    "        self.output_shape = output_shape\n",
    "        activation = nn.LeakyReLU(0.1)\n",
    "        \n",
    "        if 'num_layers' in kwargs.keys():\n",
    "            num_layers = kwargs['num_layers']\n",
    "        else:\n",
    "            num_layers = 20\n",
    "        \n",
    "        if 'layer_width' in kwargs.keys():\n",
    "            layer_width = kwargs['layer_width']\n",
    "        else:\n",
    "            layer_width = 10\n",
    "        \n",
    "        \n",
    "        layers = [nn.Linear(input_dim, layer_width), activation]\n",
    "        for i in range(num_layers-1):\n",
    "            layers.append(nn.Linear(layer_width, layer_width))\n",
    "            layers.append(activation)\n",
    "        layers.append(nn.Linear(layer_width, output_dim))\n",
    "        layers.append(activation)\n",
    "        \n",
    "        self.model = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, state, action):\n",
    "        state_action = torch.cat((state.float(), action.float()), dim=1)\n",
    "        out = self.model(state_action.float()) #reshape into a tensor of classes representing pos/vel\n",
    "            \n",
    "        return out.T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28649f7-61fd-47e8-83e2-ff615a8f254d",
   "metadata": {},
   "source": [
    "### Customized ChallengeCore module for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9950c47e-af9e-42ac-886a-19f5990ca3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomChallengeCore(Core):\n",
    "    def __init__(self, *args, action_idx=None, **kwargs):\n",
    "        if action_idx:\n",
    "            self.action_idx = action_idx\n",
    "        else:\n",
    "            self.action_idx = [0, 1]\n",
    "        super().__init__(*args, **kwargs)\n",
    "        \n",
    "    def _step(self, render):\n",
    "        \"\"\"\n",
    "        Single step.\n",
    "\n",
    "        Args:\n",
    "            render (bool):\n",
    "                whether to render or not.\n",
    "\n",
    "        Returns:\n",
    "            A tuple containing the previous state, the action sampled by the\n",
    "            agent, the reward obtained, the reached state, the absorbing flag\n",
    "            of the reached state and the last step flag.\n",
    "\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "        action = self.agent.draw_action(self._state)\n",
    "        \n",
    "        #custom CODE\n",
    "        # print('action:',action)\n",
    "        action_reshape = action.reshape(2,3)\n",
    "        end_time = time.time()\n",
    "        next_state, reward, absorbing, step_info = self.mdp.step(action_reshape[self.action_idx])\n",
    "        step_info[\"computation_time\"] = (end_time - start_time)\n",
    "\n",
    "        self._episode_steps += 1\n",
    "\n",
    "        if render:\n",
    "            self.mdp.render()\n",
    "\n",
    "        last = not (\n",
    "                self._episode_steps < self.mdp.info.horizon and not absorbing)\n",
    "\n",
    "        state = self._state\n",
    "        next_state = self._preprocess(next_state.copy())\n",
    "        self._state = next_state\n",
    "\n",
    "        return (state, action, reward, next_state, absorbing, last), step_info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91769a56-0364-4f91-bce9-0ce157a06392",
   "metadata": {},
   "source": [
    "## Setting up Training Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "75e0bb39-7f99-431d-ae12-96a9583c0022",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                                                                                                                                                                                                          \r"
     ]
    }
   ],
   "source": [
    "\n",
    "#define rewawrd function\n",
    "def custom_reward_function(base_env, state, action, next_state, absorbing):\n",
    "    reward_value = (state[0] - state[6]) * 2 + (1/abs(state[1] - state[7])) * 2 + absorbing * 3 + \\\n",
    "            (next_state[0] - next_state[6]) * 4 + (1/abs(next_state[0] - next_state[6]))\n",
    "    return reward_value\n",
    "\n",
    "\n",
    "mdp = AirHockeyChallengeWrapper(env=\"3dof-defend\", action_type=\"position-velocity\", interpolation_order=3, custom_reward_function=custom_reward_function, debug=True)\n",
    "\n",
    "\n",
    "#policy \n",
    "policy_class = OrnsteinUhlenbeckPolicy\n",
    "policy_params = dict(sigma=np.ones(1) * .2, theta=.15, dt=1e-2)\n",
    "\n",
    "\n",
    "#actor network\n",
    "actor_params = dict(network=ActionGenerator,\n",
    "                    input_shape=(12,),\n",
    "                    output_shape=(6,),\n",
    "                    loss=F.smooth_l1_loss,\n",
    "                    use_cuda=True)\n",
    "actor_optimizer = {'class': torch.optim.Adam,\n",
    "                   'params': {'lr': 0.1}}\n",
    "\n",
    "\n",
    "#critic network\n",
    "critic_input_shape = 12+6\n",
    "\n",
    "critic_params = dict(network=CriticNetwork,\n",
    "                     input_shape=(critic_input_shape,),\n",
    "                     optimizer={'class': optim.Adam,'params': {'lr': .1}},\n",
    "                     output_shape=(1,),\n",
    "                     loss=F.smooth_l1_loss,\n",
    "                     use_cuda=True)\n",
    "\n",
    "algorithm_params = {'mdp_info': mdp.info,\n",
    "                    'policy_class': policy_class,\n",
    "                    'policy_params': policy_params,\n",
    "                    'actor_params': actor_params,\n",
    "                    'actor_optimizer': actor_optimizer,\n",
    "                    'critic_params': critic_params,\n",
    "                    'batch_size': 16,\n",
    "                    'initial_replay_size': 500,\n",
    "                    'max_replay_size': 5000,\n",
    "                    'tau': 0.001 #tau = soft update coefficient\n",
    "                   }\n",
    "\n",
    "\n",
    "\n",
    "ddpg = DDPG(**algorithm_params)\n",
    "old_weights = ddpg.policy.get_weights()\n",
    "\n",
    "core = CustomChallengeCore(ddpg, mdp)\n",
    "\n",
    "core.learn(n_episodes=100, n_episodes_per_fit=100, render=True) #render allows us to visualize what's going on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "48dc1f92-f34e-4dff-a738-e0b29786e7aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.17055152,  0.07392445, -0.21482342, ...,  0.13484232,\n",
       "       -0.27873698,  0.05045644], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ddpg.policy.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee3138ed-db9d-4a27-96c7-b32a9a7b0d46",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
